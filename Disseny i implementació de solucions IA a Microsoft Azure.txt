Inicio - 22/01/2026
Profesor: Manuel Martinez
75% asistencia
https://campus.pue.es/general/prt/prg/curso.php?codcurso=C34999&btn=1

https://github.com/neuronaedgar/DissenyImplementaci-SolucionsIA


Desarrollo de soluciones de inteligencia artificial en Azure
Curso AI-102T00-A: Desarrollo de soluciones de IA en Azure
https://learn.microsoft.com/es-es/training/courses/ai-102t00

AI Engineer Exercises
https://microsoftlearning.github.io/AI-102-AIEngineer/

60 horas de curso
IA becario perfecto para un becario
Toda la RAM 2026 ya esta vendida para alimentar la IA
Automatizacion de la IA

OpenAI se hacia sobre infra de Azure.
Tengo mis datos en azure
Tenant de mi cliente voy a hacer 
La version gratuita va entrenar futuros modelos.
La version de Business enterprise 
27000 de seguridad no tienen ISO de Azure si que te lo da
OpenIA no te lo da.

Diferencia de lo Normativo entre Azure y OpenIA que no te proporciona ISO's 
Azure AIFoundry -> mismo endpoint Plataforma como servicio de AI.
Copilot -> son agentes ayudan cosas concretas.
copilot studio modelos de AIFoundry

Ingenieria de Promps -> System promp debe capar para no decir
LLM es un niño pequeño
¿Como securizas el LLM? --> es el gran agujero seguridad.

El software de futuro ya no será programado sino LLM's.

AZ-900 CURSO

Azure infra como servicio -> IaaS
PaaS -> Solución facil, te centras en el ambito del codigo (AppService)

"Azure Active Directory" y te sincronizo con tu local.
Windows 365 link -> pisa papeles solo te permite conectarte al cloud.

Reservas --> es un hardware fijo pero lo puedes devolverlo
Plan de ahorro --> te comprometes a computacion pero no al hardware

Azure App Services no acepta GPU y no hay solución de balanceo que valide la saturacion de un nodo sino que es roundrobin
Principio KISS -> appservice vs contenedor

Modelo sentimental
Modelo de IA que se pueda pagar.

ChatGPT es muy bueno pero dentro de Foundry

Microsoft Fabric_
El de F64 cuesta 4.251€

sinaps - plataforma bigdata saas fabric.
databric

aceptacion cognitiva del nuevo conocimiento.

________________________________________________________________________________________________________________
27/01/2026

Defender for identity --> Acceso Condicional
Seguridad del dato -> puerview (gobernanza del dato)
multitud de Conectores y dispone de una IA que se conecta valida los datos 

Defender for Cloud
Defender for endpoint vulnerabilidades de los equipos.

Sentinel si detecta dispositivos vulnerables aplica directivas... pues dispone de siem y soar

hay impoderables en los costes de Azure... puede existir anomalias, se debe aplicar presupuestos
Azure cost Management -> hay un delay en los costes facturados. 

Azure Arc -> amplia el management de servidores locales desde la nube.

Application insights una maravilla. Esto coges una aplicación. Y la activas e insight, y ves las tripas de la aplicación sin tener el código fuente.

Opciones en Local para jugar con la IA

La version chat gratuita no está capado

Download LM Studio (este software es hola mundo)

Es mejor un modelo pequeño optimizado

Puedes bajarte modelos desde LM Studio
Si no tienes gráfica puedes tirar del la CPU

huggingface --> comunidad de modelos
llama 3,1 8 billones de parámetros
8b Instruct para targetas de 8gB RAM

La regla es 1 billon 1GB
Mistral 3 3B es maravilloso para trastear en local (es multiidioma) --> se te baja 3GB de disco y te tiene que caber en la tarjeta gráfica.

sapphire AMD Radeon AI PRO r9700 32GB
tarjeta gráfica para IA

La IA no es determinista

Un Agente es una llamada a un LLM, donde el system Prompt que tú le has puesto es lo que tiene que hacer el agente.

Modelo:
microsoft phi 4 mini instruct (modelos limpio con muchos menos parámetros pero de mayor valor)
No te dicen el data set del entrenamiento.

Uncensored -> Para poder preguntar sin censura

https://ollama.com/

Cuando quieres montarte. Un servidor centralizado.
Que me cargue los modelos de LLM y a partir de ahí lo expongo a través de una web a P. I.
Y los diferentes programas se conectan a este por llamada Http para pedirle la respuesta.

http://localhost:11434/api/generate
{
    "model": "mistral",
    "prompt": "Why is the sky blue?"
}


1. Ciberseguridad y Gestión de Riesgos

Sistemas Legados: Se discute el riesgo de mantener hardware y software antiguos (como sistemas en COBOL o monitores de fósforo) que son críticos pero difíciles de migrar debido a la falta de recambios y conocimiento técnico.
Incidentes Reales: Se mencionan ataques de ransomware sufridos por instituciones como el Hospital Clínic, señalando que, aunque se recupere el servicio, la filtración de datos sensibles es un daño irreparable.
Estrategias de Defensa:
Zero Trust: Se explica el cambio hacia el modelo de "Confianza Cero", donde se evalúa continuamente cada intento de acceso sin importar el origen.
Defensa en Profundidad: La seguridad debe aplicarse en múltiples capas: física, identidad, red, aplicación y datos.
Herramientas de Microsoft: Se detallan servicios como Sentinel (orquestación), Defender (para identidades, endpoints y nube) e Intune (gestión de dispositivos).

2. Infraestructura y Gestión en Azure
Gobernanza y Costes: Se destaca la importancia de las etiquetas (tags) para entender la factura de Azure y los bloqueos de recursos para evitar eliminaciones accidentales o modificaciones de configuración.
Monitorización: Se menciona Application Insights para analizar el rendimiento interno de las aplicaciones y el uso de Kusto Query Language (KQL) para consultar logs.

3. Inteligencia Artificial (IA) Generativa

Modelos Locales (SLM): Se explora la posibilidad de ejecutar modelos de lenguaje pequeños (Small Language Models) en hardware local para garantizar la privacidad de los datos y ahorrar costes de tokens en la nube.
Despliegue y Hardware: Los participantes discuten el uso de herramientas como LM Studio u Ollama y la potencia de tarjetas gráficas (como la serie NVIDIA RTX 4090) para correr estos modelos con fluidez.
Adopción de Copilot: Se debate sobre la baja tasa de adopción de licencias de Copilot en las empresas y cómo la eficacia de la IA depende de que el usuario sea un perfil "senior" que sepa acotar los prompts.

4. Metodologías de Desarrollo
Se introduce el concepto de Security by Design y cómo integrar pruebas de ciberseguridad dentro del ciclo de TDD (Test Driven Development) para que ninguna funcionalidad nueva se entregue sin ser segura.
________________________________________________________________________________________________________________
29/01/2026

ML Net
GPT 4.1 para codigo es muy  y es gratuito
Onnix Runtime GenAI genial para insertar un slm en tu codigo
https://huggingface.co/
Modelo instruido --> son modelos supervisado y solo carga INFO relevante

Examen
https://www.examprepper.co/

Efecto Dunning - Kruger --> el monte de la ignorancia
Nivel de competencia Real vs Nivel competencia Auto-percibida
Cima del "Monte de la Ignorancia"

Token palabra independiente

LLM es un horaculo de palabras... 
Un SLM instruido por los expertos de Matemáticas no tiene basura.
Gran modelo 
Modelos instruidos con informacion apropiada.
GhatGPT se ha entrenado con mucha basura

IA Generativa es salida con una respuesta generada

La magia esta en el system promp
El modelo debe ser securizado para que un system promp no pueda ser modificado por un user promp.
Promt - aviso sistema: 

https://www.lakera.ai/lakera-gandalf
https://github.com/ZapDos7/lakera-gandalf

Los LLM no tiene memoria... El historial es el reenvio de lo almacenado

Microsoft Foundry -> repositorio poder instanciar endpoint LLM. 
OpenAI 
Pagar por tokens
Agentes que puedan automatizar n8n
  
Laboratorios:
https://labs.xtremelabs.io/RedeemCode/Redeem?ap=MyLabs&code=moc-q62wb32gz

tienes 2 horas para hacer pruebas

1. Aplicación Práctica de Visión Artificial

Entrenamiento Supervisado: Se realiza una demostración de clasificación de imágenes (frutas como kiwis y tomates) utilizando un dataset etiquetado en carpetas.
Ciclo de Desarrollo: Se explica el proceso desde la creación del dataset, el inicio del entrenamiento (que duró aproximadamente 65 segundos para 25 imágenes por categoría) hasta la fase de inferencia o predicción.
Modelo Utilizado: Se aclara que, aunque parece machine learning tradicional, se está utilizando Deep Learning (aprendizaje profundo) mediante una red neuronal convolucional basada en Tensorflow y una técnica de Transfer Learning con el modelo YOLO.
Caso de Uso Real: Manuel comparte un ejemplo personal donde usó un Arduino con cámara para monitorizar un termostato analógico de una caldera. Entrenó un modelo para reconocer la temperatura mediante fotos y activar una electroválvula automáticamente.

2. Despliegue y Consumo de Modelos

Publicación: Una vez entrenado el modelo, se discute cómo consumirlo a través de una aplicación de consola o publicarlo como una Web API en la nube (Azure App Service) o en servidores locales de la empresa.
Modelos en Local (SLM): Se explora el uso de modelos de lenguaje pequeños (Small Language Models) ejecutados localmente para mayor eficiencia y privacidad, recomendando herramientas como ONNX Runtime frente a otras opciones como Ollama.


3. Arquitectura y Estrategias de IA Corporativa

RAG (Retrieval-Augmented Generation): Se debate sobre cómo alimentar a la IA con datos específicos de la empresa utilizando bases de datos vectoriales y servicios como Azure AI Search para indexar documentos confidenciales y mejorar las respuestas del agente.
Gestión de Tokens y Contexto: Se explica que el "contexto" son las palabras de entrada y salida, y se mencionan estrategias para reducir costes de tokens, como el uso de índices en lugar de enviar documentos enteros en cada petición.
Validación y Pruebas: Ante cambios de modelos, se recomienda realizar pruebas en paralelo y baterías de validación para asegurar que la nueva versión del modelo no degrade la calidad de las respuestas anteriores.

4. Ética e Impacto Laboral

IA como Asistente: Se refuerza la idea de que la IA no reemplaza a los programadores o expertos, sino que los ayuda a automatizar tareas repetitivas de bajo valor añadido, permitiéndoles centrarse en la especialización y la supervisión humana.
Responsabilidad: Se subraya la importancia de la transparencia, indicando siempre al usuario cuándo está interactuando con una IA.

________________________________________________________________________________________________________________
03/02/2026
Lab: AI-102T00-A-CEP-M01L01

En esta clase, el enfoque fue eminentemente práctico, centrándose en la implementación de soluciones de IA en Azure y la interacción con modelos de lenguaje.

Aquí tienes el resumen de los puntos clave tratados:

1. Prácticas en el Laboratorio (Azure AI Services)
Configuración de Recursos: Gran parte del inicio se dedica a la creación y despliegue de recursos de Azure OpenAI y AI Services.

Modelos de Lenguaje: Se trabaja con modelos como GPT-3.5 Turbo y GPT-4, configurando parámetros como la Temperatura (para controlar la creatividad vs. determinismo) y las Tokens (límite de respuesta).

Seguridad y Despliegue: Se discute la importancia de las "Content Filters" (filtros de contenido) para evitar respuestas inapropiadas en entornos corporativos.

2. Ingeniería de Prompts (Prompt Engineering)
System Message (Mensaje de Sistema): Se explica cómo definir el comportamiento del modelo (ej. "Eres un asistente experto en marketing") para acotar las respuestas.

Few-Shot Learning: Manuel muestra cómo dar ejemplos al modelo dentro del prompt para que aprenda el formato o tono deseado antes de responder.

Chat vs. Completado: Se analiza la diferencia entre modelos diseñados para seguir una conversación y modelos para completar texto.

3. Desarrollo e Integración (C# y Python)
Consumo de API: Los participantes ven cómo llevar lo probado en el "Playground" de Azure a código real utilizando el SDK de Azure OpenAI.

Variables de Entorno: Se recalca la buena práctica de no escribir las claves de API directamente en el código (hardcoding), sino usar variables de entorno o archivos de configuración.

Flujo de Trabajo: Se explica que la IA suele ser una parte pequeña de una aplicación más grande, donde el programa hace una petición, recibe un JSON y luego procesa esa información para el usuario final.

4. Inteligencia Artificial Responsable
Sesgos y Alucinaciones: Se debate sobre la tendencia de los modelos a "inventar" información de manera muy convincente y la necesidad de la supervisión humana.

Privacidad de Datos: Se asegura que, al usar Azure OpenAI, los datos enviados no se utilizan para entrenar los modelos globales de OpenAI, garantizando la privacidad empresarial.

5. Temas Varios y Debates
Adopción Tecnológica: Existe un debate sobre la velocidad a la que avanzan estas herramientas y la dificultad de las empresas para mantener el ritmo de actualización.

Ciberseguridad: Se mencionan de paso incidentes recientes (como el de Cloudflare o temas de piratería en el fútbol) como ejemplos de la importancia de la seguridad en la infraestructura actual.
________________________________________________________________________________________________________________
05/02/2026
https://jsfuck.com/#  (meter codigo oculto html)

https://msfthub.com/labs/azure/ai-102/

Concepto de RAG
Si nos cobran por token, 
Cogemos un servicio cognitivo de Azure, te montas una cache para futuras preguntas.
Azure AI search y crea un indice semantico de busqueda

El problema no están en el coste de particionado sino en la cantidad de datos que tienes al LLM para devolverte la respuesta.

Una PTU (Processing Token Unit) es una unidad de capacidad de procesamiento reservada que garantiza un nivel 
determinado de rendimiento (throughput) para la ejecución de modelos de inteligencia artificial,
permitiendo asegurar latencia, concurrencia y costes predecibles en entornos productivos.

Laboratorio:
AI-102T00-A-CEP-M01L03

Username
XLab-N5Y-056@xtremelabs.us 
Temporary Access Pass
j6vw^8@z 

git clone https://github.com/microsoftlearning/mslearn-ai-studio mslearn-ai-foundry
cd mslearn-ai-foundry/labfiles/chat-app/python
#crear un entorno virtual (virtual environment) de Python
python -m venv labenv  
# Cambia el intérprete de Python activo al del entorno
./labenv/bin/Activate.ps1
# Instalar las dependencias definidas requirements.txt y librerías azure-identity, azure-ai-projects y openai
pip install -r requirements.txt azure-identity azure-ai-projects openai
# abre un editor como un vi
code .env
code chat-app.py

# Add references
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from openai import AzureOpenAI

# Initialize the project client
project_client = AIProjectClient(            
        credential=DefaultAzureCredential(
            exclude_environment_credential=True,
            exclude_managed_identity_credential=True
        ),
        endpoint=project_endpoint,
    )

# Get a chat client
openai_client = project_client.get_openai_client(api_version="2024-10-21")

prompt = [ {"role": "system", "content": "You are a helpful AI assistant that answers questions."}]

# Get a chat completion
prompt.append({"role": "user", "content": input_text})
   response = openai_client.chat.completions.create(
            model=model_deployment,
            messages=prompt)
completion = response.choices[0].message.content
print(completion)
prompt.append({"role": "assistant", "content": completion})


AI-102T00-A-CEP-M01L04
Username=XVUser12084@xtremelabs.us 
Password=U&8xK89t 
AI hub resource


Recurso de Microsoft Foundry - Microsoft Foundry resource
Recurso del centro de IA - AI hub resource
________________________________________________________________________________________________________________
10/02/2026

Security by default en IA no existe










